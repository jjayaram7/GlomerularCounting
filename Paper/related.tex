\subsection{Graph Embedding - Prior Art}

\subsection{Building Affinities for Union of Subspaces}

The performance of several unsupervised, supervised, and semi-supervised learning procedures used in computer vision problems can be improved when the high-dimensional data is mapped to an appropriate low-dimensional space. Depending on the application, these dimensionality reduction approaches can be used to aid in visualization or to identify meanining clusters. In addition, by providing suitable supervisory information improved class discrimination can be achieved. Subspace methods such as principal components analysis (PCA) attempt to identify the low-dimensional subspace that preserves maximum information about the data. However, when the data is not distributed along low-dimensional subspaces, the utility of traditional subspace methods diminish. Furthermore, the inherent geometry of data in several applications can be non-linear \cite{chang2006manifold,seung2000manifold}. For example, a set of images generated by rotating or scaling a template image, reside on a low-dimensional, non-linear manifold parametrized by the scale and rotation parameters, and not on a subspace. In such cases, we can employ non-linear dimensionality reduction (NLDR) schemes such as manifold learning methods to infer the underlying intrinsic geometric structure of the data. Subspace and manifold based dimensionality reduction methods can also be broadly categorized based on whether they preserve the global or the local structure of data. A class of approaches known as manifold clustering and manifold learning algorithms such as locally linear embedding \cite{Saul00anintroduction}, Hessian LLE \cite{Donoho03hessianeigenmaps} and Laplacian Eigenmaps \cite{Belkin01laplacianeigenmaps} preserve local information of the manifold. Global methods such as ISOMAP \cite{isomaps} and semidefinite embedding \cite{Weinberger04} try to preserve both global and local relationships. While a variety of such methods have been proposed, how the low-dimensional map can be computed for a novel test sample is an important question that comes up in all these approaches. Another key consideration is whether the dimensionality reduction methods are able to exploit the label information that may be available in the training data. These two factors may  potentially limit the use of several such methods in supervised and semi-supervised settings. Some algorithms address this challenge partially by finding a mapping for the whole data space, not just for the training samples \cite{athitsos2004boostmap,he2003learning,roweis2002global,lowe1997neuroscale}. However, they do not consider the label information available with the training data and hence do not result in an improved discrimination between classes. 

Several dimensionality reduction and learning schemes can be unified under the general framework of graph embedding \cite{yan2007graph}. In this approach, the relationship between data samples are coded using graphs and spectral methods are used to perform dimensionality reduction. Some well-known schemes that can be posed as graph embedding problems include PCA, ISOMAP \cite{isomaps}, locality preserving projections (LPP) \cite{LPP}, LLE \cite{Saul00anintroduction}, Laplacian eigenmaps \cite{Belkin01laplacianeigenmaps} , linear discriminant analysis (LDA) \cite{keinosuke1990introduction}, local discriminant embedding (LDE) \cite{chen2005local}, semi-supervised discriminant analysis (SDA) \cite{cai2007semi} and several others. Graph embedding for unsupervised learning uses one graph that encodes the similarity between data samples. In supervised and semi-supervised learning problems, two graphs are defined to encode the intra- and inter- class relationships respectively. When the local neighborhood of the data sample is used to construct a graph, we obtain a \textit{local graph}. Two approaches are commonly used for constructing local graphs: (i) nearest-neighbor method, where, for each data sample, $k$ nearest neighbors are chosen, and (ii) $\epsilon$-ball based method, where, for each data sample, the samples lying in the $\epsilon$ ball surrounding it are chosen. In both cases, the graph edge weights can be fixed as binary values, Gaussian kernel values or the Euclidean distances directly.

Though local graphs have been successful in several scenarios, their performance can be affected because of: (i) their lack of robustness to noise, (ii) their inability to adapt the neighborhood parameters according to each data sample, and (iii) non-suitability of the locality assumption. Local graphs are non-robust because the neighbors are identified based on Euclidean distance, and noise in even a few data samples, particularly when it is non-Gaussian, can change the graph structure significantly. Furthermore, when the data is not uniformly sampled across different regions in space, using conventional graph construction approaches with a global neighborhood parameter ($k$ or $\epsilon$) may lead to suboptimal graphs. However, the most important reason why the local graph construction may result in a poor performance, is when the assumption of locality itself is unsuitable to the distribution under consideration. For example, the data may be clustered along unions of low-dimensional subspaces, in which case constructing graphs using Euclidean distances may not result in good clusterings.

The natural way of representing data that lie along unions of low dimensional subspaces is to use sparse coding \cite{Elad_book}. The sparse code for each data sample is obtained by expressing it as a linear combination of few elements from the dictionary matrix. Sparse coding based graphs can be constructed using a predefined dictionary or a set of examples \cite{cheng2010,ramirez2010classification}. When the training samples themselves are used as the dictionary, we refer to the graph obtained as the $\ell_1$ graph. The $\ell_1$ graph is non-local since the graph relationships inferred do not depend on the local neighborhood. It has been demonstrated that sparse coding based graphs are robust to noise, and do not include unrelated inhomogeneous data since the choice of neighbors is data dependent. However, constructing the $\ell_1$ graph is computationally expensive compared to constructing a local graph. Furthermore, sparse coding graph constructions have been unsupervised so far, and do not take into account the label information of the training data.

It is important to understand the framework of embedding using supervised local graphs, before discussing our proposed non-local supervised graph embedding method. To begin with, we will consider the locality preserving projections (LPP) approach, which is an unsupervised embedding approach for local graphs that computes projection directions such that the pairwise distances of the projected training samples in the neighborhood are preserved. Let us define the training data as $\{\mathbf{x}_i|\mathbf{x}_i \in \mathbb{R}^M\}_{i=1}^T$. An undirected graph $G$ is defined, with the training samples as vertices, and the similarity between the neighboring training samples are coded in the affinity matrix $\mathbf{W} \in \mathbb{R}^{T \times T}$.  Let us denote the graph Laplacian as $\mathbf{L} = \mathbf{D}-\mathbf{W}$, where $\mathbf{D}$ is a degree matrix with each diagonal element containing the sum of the corresponding row or column of $\mathbf{L}$. The $d$ projection directions for LPP, $\mathbf{V} \in \mathbb{R}^{M \times d}$, where $d < M$, can be computed by optimizing
\begin{align}
\min_{\text{trace}(\mathbf{V}^T \mathbf{X} \mathbf{D}\mathbf{X}^T\mathbf{V}) = \mathbf{I}} \text{trace}(\mathbf{V}^T \mathbf{X} \mathbf{L}\mathbf{X}^T\mathbf{V}).
\label{eqn:lpp}
\end{align}Here $\mathbf{X}$ is a matrix obtained by stacking all data samples as its columns. The embedding for any data sample $\mathbf{x}$ can be obtained by projecting it onto the orthonormal directions $\mathbf{V}$ as $\mathbf{V}^T \mathbf{x}$. It can be shown that LPP computes the projection directions by minimizing the objective 
\begin{equation}
\min_{\mathbf{V}} \sum_{i,j=1}^T \|\mathbf{V}^T \mathbf{x}_i - \mathbf{V}^T \mathbf{x}_j\|_2^2 w_{ij} \text{  s.t.  } \sum_{i=1}^T \|\mathbf{V}^T \mathbf{x}_i\|_2^2 \delta_{ij} =1.
\label{eqn:lppopt}
\end{equation}This optimization ensures that the embedding preserves the neighborhood structure of the graph.

However, when class label information is available, incorporating it will result in embeddings that can discriminate different classes of data. Linear Discriminant Analysis (LDA) is such a supervised graph embedding framework that uses the within-class and between-class scatter matrices to obtain discriminative projections. Let us assume that the class labels corresponding to the set of training samples, $\{\mathbf{x}_i|\mathbf{x}_i \in \mathbb{R}^M\}_{i=1}^T$, are known and denoted by $\{y_i|y_i \in \{1,2,\ldots,C\}\}_{i=1}^T$, where $C$ indicates the total number of classes. 

In order to pose LDA as a graph embedding problem, we compute the intra-class graph weights as,
\begin{equation}
w_{ij} = 
\begin{cases}
\frac{1}{T_{y_i}} & \text{if } y_i = y_j, \\
0 & \text{otherwise},
\end{cases}
\label{eqn:ldaintra}
\end{equation}where $T_{y_i}$ denotes the total number of samples with the label $y_i$. These weights are elements of the intra-class graph affinity or weight matrix $\mathbf{W}$. The inter-class graph weights are obtained as
\begin{equation}
w_{ij}' = \frac{1}{T},
\label{eqn:ldainter}
\end{equation} and they are the elements of the inter-class graph affinity matrix $\mathbf{W}'$. Using the intra- and inter-class graphs, we also construct the diagonal degree matrices $\mathbf{D}$ and $\mathbf{D}'$, whose elements are obtained as $d_{ii} = \sum_j w_{ij}$ and $d_{ii}' = \sum_j w_{ij}'$ respectively. Finally, the graph Laplacian matrices are obtained as $\mathbf{L} = \mathbf{D} - \mathbf{W}$ and $\mathbf{L}' = \mathbf{D}' - \mathbf{W}'$. Given the two graph Laplacian matrices, the low-dimensional embedding is computed by optimizing
\begin{equation}
\max_{\mathbf{V}} \frac{\text{Tr}[\mathbf{V}^T \mathbf{X}^T \mathbf{L}' \mathbf{X} \mathbf{V}]}{\text{Tr}[\mathbf{V}^T \mathbf{X}^T \mathbf{L} \mathbf{X} \mathbf{V}]}.
\label{eqn:ldaopt}
\end{equation}Note that, in general, the dimension of the low-dimensional embedding $d$ is fixed at $C-1$, in supervised graph embedding. Eqn. (\ref{eqn:ldaopt}) obtains projection directions such that the between-class scatter is maximized, whereas the within-class scatter is minimized in the embedded data. Instead of finding the global solution to the trace-ratio maximization problem in (\ref{eqn:ldaopt}), a greedy solution can be obtained by converting it to an equivalent ratio-trace maximization, $\max_{\mathbf{V}} \text{Tr}[(\mathbf{V}^T \mathbf{X}^T \mathbf{L} \mathbf{X} \mathbf{V})^{-1}\mathbf{V}^T \mathbf{X}^T \mathbf{L}' \mathbf{X} \mathbf{V}]$. The solution to this problem can be obtained using the generalized eigen value decomposition
\begin{equation}
\mathbf{X} \mathbf{L}' \mathbf{X}^T = \lambda \mathbf{X} \mathbf{L} \mathbf{X}^T,
\label{eqn:geneig}
\end{equation}where the projection directions $\mathbf{V}$ are the $d$ top eigen vectors.

Apart from the class label information, the local structure of the data samples can also be incorporated when constructing the affinity matrices. Let $\mathcal{N}_k(i)$ denote the set of $k-$nearest neighbors for the data sample $\mathbf{x}_i$. The elements of the intra- and inter-class affinity matrices are respectively obtained as
\begin{equation}
w_{ij} = 
\begin{cases}
1 & \text{if } y_i = y_j \text{ AND } [i \in \mathcal{N}_k(j) \text{ OR } j \in \mathcal{N}_k(i)], \\
0 & \text{otherwise}.
\end{cases}
\label{eqn:ldeintra}
\end{equation}
\begin{equation}
w_{ij}' = 
\begin{cases}
1 & \text{if } y_i \neq y_j \text{ AND } [i \in  \mathcal{N}_k'(j) \text{ OR } j \in \mathcal{N}_k'(i)], \\
0 & \text{otherwise}.
\end{cases}
\label{eqn:ldeinter}
\end{equation}Using these affinity matrices, projection matrices for embeddings can be computed using ratio-trace maximization similar to LDA, and this approach is referred to as local discriminant embedding (LDE).
